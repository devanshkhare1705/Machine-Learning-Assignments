{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "s_rEtghp-BkC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHuRe8943V84"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy, matplotlib.pyplot as plt, scipy\n",
        "from scipy.stats import entropy\n",
        "from sklearn import tree"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 a) Data Loader"
      ],
      "metadata": {
        "id": "IKM4pkME-L7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(clean_real_text,clean_fake_text):\n",
        "    combined_text = []\n",
        "    labels = []\n",
        "    cv = CountVectorizer()\n",
        "\n",
        "    with open (clean_real_text,'r') as real:\n",
        "        lines = real.readlines()  #to avoid IO text wrapping errors\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            combined_text.append(line)\n",
        "            labels.append(1)\n",
        "\n",
        "    with open (clean_fake_text) as fake:\n",
        "        lines = fake.readlines()   #to avoid IO text wrapping errors\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            combined_text.append(line)\n",
        "            labels.append(0)\n",
        "\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(combined_text, labels, test_size = 0.15,train_size = 0.85) #X and Y have to be the same size\n",
        "    X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size = 0.1765, train_size = 0.8235)\n",
        "\n",
        "    vector_training = cv.fit_transform(X_train).toarray()\n",
        "    vector_validation = cv.transform(X_validation)\n",
        "    vector_test = cv.transform(X_test)\n",
        "    feature_names = cv.get_feature_names_out()\n",
        "    feature_names = feature_names.tolist()\n",
        "    outputs = [vector_training, Y_train, vector_validation, Y_validation, vector_test, Y_test, feature_names]\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "vXAnTWW53rQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimal Decision Tree Classifier"
      ],
      "metadata": {
        "id": "vwYwYmEpCGIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_model(data, max_depths, split_criteria):\n",
        "    max_depth_accuracies = []; model_characteristics = []; tree_models = [] #model_characteristics = [[\"max_depth\",\"split\",\"accuracy\",\"model\"]]\n",
        "\n",
        "    for criteria in split_criteria:\n",
        "        for depth in max_depths:\n",
        "\n",
        "            #train decision tree\n",
        "            model = DecisionTreeClassifier(criterion = criteria,max_depth = depth)  #initialize decision tree\n",
        "            model.fit(data[0],data[1])   #train decision tree\n",
        "\n",
        "            #validation accuracy assessment\n",
        "            predictions = model.predict(data[2])\n",
        "            model_accuracy = accuracy_score(data[3],predictions)\n",
        "\n",
        "            #recording validation accuracies\n",
        "            max_depth_accuracies.append(model_accuracy)   #set of all validation accuracies\n",
        "            tree_models.append(model)\n",
        "            model_characteristics.append([depth,criteria,model_accuracy,model])   #set of characteristics of all trees\n",
        "\n",
        "\n",
        "    max_accuracy_index = max_depth_accuracies.index(max(max_depth_accuracies))  #find index of highest accuracy\n",
        "    selected_model = tree_models[max_accuracy_index]\n",
        "    selected_model_characteristics = model_characteristics[max_accuracy_index]\n",
        "\n",
        "    print(\"Accuracies for each model =\", max_depth_accuracies)\n",
        "\n",
        "    #plot for Information Gain Criteria\n",
        "    plt.plot(max_depths,max_depth_accuracies[0:5])\n",
        "    plt.xlabel('Max Depth')\n",
        "    plt.ylabel('Validation Accuracy (Information Gain Criteria)')\n",
        "    plt.show()\n",
        "\n",
        "    #plot for Gini Criteria\n",
        "    plt.plot(max_depths,max_depth_accuracies[5:10])\n",
        "    plt.xlabel('Max Depth')\n",
        "    plt.ylabel('Validation Accuracy (Gini Criteria)')\n",
        "    plt.show()\n",
        "\n",
        "    #plot for Log Loss Criteria\n",
        "    plt.plot(max_depths,max_depth_accuracies[10:15])\n",
        "    plt.xlabel('Max Depth')\n",
        "    plt.ylabel('Validation Accuracy (Log_Loss Criteria')\n",
        "    plt.show()\n",
        "\n",
        "    return selected_model_characteristics"
      ],
      "metadata": {
        "id": "Wtc9vYGT3rgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Visualiser"
      ],
      "metadata": {
        "id": "V-J9BxriCf0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "import graphviz\n",
        "\n",
        "def visualizer(model, feature_names):\n",
        "    dot_format = tree.export_graphviz(model, feature_names = feature_names)\n",
        "    graph = graphviz.Source(dot_format)\n",
        "\n",
        "    return graph\n",
        "\n",
        "visualizer(Decision_Tree)"
      ],
      "metadata": {
        "id": "Iho-eOmA3sQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain Function"
      ],
      "metadata": {
        "id": "YgIS88JDCj16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_information_gain(dataset, keyword_xi, feature_names):\n",
        "    X_training = dataset[0]; Y_training = dataset[1];\n",
        "\n",
        "    probability_real = Y_training.count(1)/len(Y_training) #Find probability of real in training dataset\n",
        "    h = entropy([probability_real, 1 - probability_real], base = 2) #Find entropy of the entire training dataset\n",
        "\n",
        "    absence_indicator = []\n",
        "    presence_indicator = []\n",
        "    keyword_index = feature_names.index(keyword_xi) #get position of keyword assigned during vectorization\n",
        "    keyword_presence_indicators = X_training[:,keyword_index] #get count of keywords in each line\n",
        "\n",
        "    for indicator in keyword_presence_indicators:\n",
        "            absence_indicator.append(indicator==0)  #collection of absence indication for all lines\n",
        "            presence_indicator.append(indicator!=0) #collection of presence indication for all lines\n",
        "\n",
        "    probability_absent = sum(absence_indicator)/len(absence_indicator)\n",
        "    probability_present = sum(presence_indicator)/len(presence_indicator)\n",
        "    if probability_absent == 0:\n",
        "        return 0\n",
        "    elif probability_absent == 1:\n",
        "        return 0\n",
        "\n",
        "    Y_training = numpy.array(Y_training)\n",
        "    prob_real_given_absent = Y_training[absence_indicator].sum()/len(Y_training[absence_indicator])\n",
        "    prob_real_given_present = Y_training[presence_indicator].sum()/len(Y_training[presence_indicator])\n",
        "    h_real_given_absent = entropy ([prob_real_given_absent, 1-prob_real_given_absent], base = 2)\n",
        "    h_real_given_present = entropy ([prob_real_given_present, 1-prob_real_given_absent], base = 2)\n",
        "\n",
        "    info_gain = h - probability_present*h_real_given_present - probability_absent*h_real_given_absent\n",
        "    return info_gain\n"
      ],
      "metadata": {
        "id": "0gpVWMjt4HMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Function"
      ],
      "metadata": {
        "id": "72v3GxnWC0L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  information_gained = []\n",
        "\n",
        "  max_depths = [1,5,20,50,100]\n",
        "\n",
        "  data = load_data('clean_real.txt','clean_fake.txt')\n",
        "\n",
        "  feature_names = data[6]\n",
        "\n",
        "  split_criteria = [\"entropy\", \"gini\", \"log_loss\"]\n",
        "\n",
        "  model = select_model(data, max_depths, split_criteria)[3]\n",
        "  diagram = visualizer(model, feature_names)\n",
        "\n",
        "  keywords = [\"the\",\"hillary\",\"donald\"]\n",
        "  for keyword in keywords:\n",
        "    information_gained.append(compute_information_gain(data,keyword,feature_names))\n",
        "\n",
        "  print(\"Information Gained = \", information_gained)"
      ],
      "metadata": {
        "id": "TeRKAe4cC0XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ = \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "cvpzTEkTEQF_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
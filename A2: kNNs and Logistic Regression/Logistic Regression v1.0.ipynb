{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UayW-mYCT339"
      },
      "outputs": [],
      "source": [
        "from utils import sigmoid\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def logistic_predict(weights, data):\n",
        "    \"\"\" Compute the probabilities predicted by the logistic classifier.\n",
        "\n",
        "    Note: N is the number of examples\n",
        "          M is the number of features per example\n",
        "\n",
        "    :param weights: A vector of weights with dimension (M + 1) x 1, where\n",
        "    the last element corresponds to the bias (intercept).\n",
        "    :param data: A matrix with dimension N x M, where each row corresponds to\n",
        "    one data point.\n",
        "    :return: A vector of probabilities with dimension N x 1, which is the output\n",
        "    to the classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    bias_column = np.ones((data.shape[0],1))\n",
        "    X_updated = np.concatenate((data, bias_column),axis = 1)\n",
        "    z = np.dot(X_updated,weights)\n",
        "    y = sigmoid(z)\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(targets, y):\n",
        "    \"\"\" Compute evaluation metrics.\n",
        "\n",
        "    Note: N is the number of examples\n",
        "          M is the number of features per example\n",
        "\n",
        "    :param targets: A vector of targets with dimension N x 1.\n",
        "    :param y: A vector of probabilities with dimension N x 1.\n",
        "    :return: A tuple (ce, frac_correct)\n",
        "        WHERE\n",
        "        ce: (float) Averaged cross entropy\n",
        "        frac_correct: (float) Fraction of inputs classified correctly\n",
        "    \"\"\"\n",
        "    t = np.array(targets).T\n",
        "    y = np.array(y)\n",
        "    ce = np.average(-np.dot(t,np.log(y))-np.dot((1-t),np.log(1-y)))      #cross entropy; returns a scalar\n",
        "\n",
        "    sum1 = np.sum((abs(targets-y) < 0.5))    #returns a scalar\n",
        "    sum2 = np.sum((y==1)*(abs(targets-y == 0.5))) #edge case where diff of 0.5 counts as correct if t = 1\n",
        "\n",
        "    frac_correct = (sum1+sum2)/y.size\n",
        "\n",
        "    return ce, frac_correct"
      ],
      "metadata": {
        "id": "hfb7GkDfUG0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic(weights, data, targets, hyperparameters):\n",
        "    \"\"\" Calculate the cost and its derivatives with respect to weights.\n",
        "    Also return the predictions.\n",
        "\n",
        "    Note: N is the number of examples\n",
        "          M is the number of features per example\n",
        "\n",
        "    :param weights: A vector of weights with dimension (M + 1) x 1, where\n",
        "    the last element corresponds to the bias (intercept).\n",
        "    :param data: A matrix with dimension N x M, where each row corresponds to\n",
        "    one data point.\n",
        "    :param targets: A vector of targets with dimension N x 1.\n",
        "    :param hyperparameters: The hyperparameter dictionary.\n",
        "    :returns: A tuple (f, df, y)\n",
        "        WHERE\n",
        "        f: The average of the loss over all data points.\n",
        "           This is the objective that we want to minimize.\n",
        "        df: (M + 1) x 1 vector of derivative of f w.r.t. weights.\n",
        "        y: N x 1 vector of probabilities.\n",
        "    \"\"\"\n",
        "    y = logistic_predict(weights, data)\n",
        "    f, fraction = evaluate(targets,y)\n",
        "    dw = np.dot((y-targets).T,data)\n",
        "    db = np.dot((y-targets).T,np.ones((y.shape[0],1)))\n",
        "    df = np.concatenate((dw,db),axis = 1).T\n",
        "\n",
        "    return f, df, y"
      ],
      "metadata": {
        "id": "ecQDTYe_UOHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from check_grad import check_grad\n",
        "from utils import *\n",
        "from logistic import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def run_check_grad(hyperparameters):\n",
        "    \"\"\" Performs gradient check on logistic function.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    # This creates small random data with 20 examples and\n",
        "    # 10 dimensions and checks the gradient on that data.\n",
        "    num_examples = 20\n",
        "    num_dimensions = 10\n",
        "\n",
        "    weights = np.random.randn(num_dimensions + 1, 1)\n",
        "    data = np.random.randn(num_examples, num_dimensions)\n",
        "    targets = np.random.rand(num_examples, 1)\n",
        "\n",
        "    diff = check_grad(logistic,\n",
        "                      weights,\n",
        "                      0.001,\n",
        "                      data,\n",
        "                      targets,\n",
        "                      hyperparameters)\n",
        "\n",
        "    print(\"diff =\", diff)"
      ],
      "metadata": {
        "id": "6dnpM1f6Uyli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_logistic_regression():\n",
        "    train_inputs, train_targets = load_train()\n",
        "    valid_inputs, valid_targets = load_valid()\n",
        "    test_inputs, test_targets = load_test()\n",
        "\n",
        "    N, M = train_inputs.shape\n",
        "\n",
        "    hyperparameters = {\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"weight_regularization\": 0,\n",
        "        \"num_iterations\": 50\n",
        "    }\n",
        "    weights = np.ones((M+1,1))\n",
        "    weights-=0.8\n",
        "\n",
        "\n",
        "    # Verify that logistic function produces the right gradient.\n",
        "    # diff should be very close to 0.\n",
        "    run_check_grad(hyperparameters)\n",
        "\n",
        "\n",
        "    # Begin learning with gradient descent\n",
        "    train_ce =[]\n",
        "    train_accuracy = []\n",
        "    val_ce = []\n",
        "    val_accuracy = []\n",
        "    its = hyperparameters[\"num_iterations\"]\n",
        "    lr = hyperparameters[\"learning_rate\"]\n",
        "\n",
        "    for t in range(its):\n",
        "        train_J,train_dJ,train_y = logistic(weights,train_inputs,train_targets,hyperparameters)\n",
        "        train_cr_en, train_acc = evaluate(train_targets, train_y)\n",
        "\n",
        "        train_ce.append(train_cr_en)\n",
        "        train_accuracy.append(train_acc)\n",
        "\n",
        "        weights-=lr*train_dJ\n",
        "\n",
        "        val_J, val_dj, val_y = logistic(weights,valid_inputs,valid_targets, hyperparameters)\n",
        "        val_cr_en, val_acc = evaluate(valid_targets, val_y)\n",
        "\n",
        "        val_ce.append(val_cr_en)\n",
        "        val_accuracy.append(val_acc)\n",
        "\n",
        "    test_y = logistic_predict(weights,test_inputs)\n",
        "    test_ce, test_accuracy = evaluate(test_targets, test_y)\n",
        "    print(\"test_accuracy = \", test_accuracy)\n",
        "\n",
        "\n",
        "    plt.figure(0)\n",
        "    plt.plot(range(1,its+1),train_ce,label = \"training cross entropy\")\n",
        "    plt.plot(range(1,its+1),val_ce,label = \"validation cross entropy\")\n",
        "    plt.title(\"mnist_train: Cross Entropy vs. Iterations\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"cross entropy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.plot(range(1,its+1),train_accuracy,label = \"training accuracy\")\n",
        "    plt.plot(range(1,its+1),val_accuracy,label = \"validation accuracy\")\n",
        "    plt.title(\"mnist_train: Accuracy vs. Iterations\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    return test_ce, test_accuracy"
      ],
      "metadata": {
        "id": "8kqk823aUaRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    run_logistic_regression()"
      ],
      "metadata": {
        "id": "4tIkFRjyVKDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}